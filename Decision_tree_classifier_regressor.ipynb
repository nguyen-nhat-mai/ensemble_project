{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyen-nhat-mai/ensemble_project/blob/main/Decision_tree_classifier_regressor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DECISION TREE CLASSIFIER and REGRESSOR**\n",
        "By Haiwei FU, Mengyu LIANG, Nhat Mai NGUYEN, Jinji SHEN and Vanshika SHARMA"
      ],
      "metadata": {
        "id": "h1Ky08SBpnOS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Setup**"
      ],
      "metadata": {
        "id": "Rjjs4_qJqDxG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pdTEiEdhpfKC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from scipy import stats\n",
        "from collections import Counter\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.datasets import load_breast_cancer,make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score,mean_squared_error,mean_absolute_error,r2_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.tree import DecisionTreeRegressor"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Self-build DT**"
      ],
      "metadata": {
        "id": "DDSOllQN1ueA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#class to control tree node\n",
        "class Node:\n",
        "    #initializer\n",
        "    def __init__(self):\n",
        "        self.__Bs    = None\n",
        "        self.__Bf    = None\n",
        "        self.__left  = None\n",
        "        self.__right = None\n",
        "        self.leafv   = None\n",
        "    #set the split,feature parameters for this node\n",
        "    def set_params(self,Bs,Bf):\n",
        "        self.__Bs = Bs\n",
        "        self.__Bf = Bf\n",
        "        \n",
        "    #get the split,feature parameters for this node\n",
        "    def get_params(self):\n",
        "        return(self.__Bs,self.__Bf)    \n",
        "        \n",
        "    #set the left/right children nodes for this current node\n",
        "    def set_children(self,left,right):\n",
        "        self.__left  = left\n",
        "        self.__right = right\n",
        "        \n",
        "    #get the left child node\n",
        "    def get_left_node(self):\n",
        "        return(self.__left)\n",
        "    \n",
        "    #get the right child node\n",
        "    def get_right_node(self):\n",
        "        return(self.__right)\n",
        "\n",
        "#base class to encompass the decision tree algorithm\n",
        "class DecisionTree:\n",
        "    #initializer\n",
        "    def __init__(self,max_depth=None,min_samples_split=2):\n",
        "        self.tree              = None\n",
        "        self.max_depth         = max_depth\n",
        "        self.min_samples_split = min_samples_split\n",
        "    \n",
        "    #protected function to define the impurity\n",
        "    def _impurity(self,D):\n",
        "         pass\n",
        "        \n",
        "    #protected function to compute the value at a leaf node\n",
        "    def _leaf_value(self,D):\n",
        "         pass\n",
        "    \n",
        "    #private recursive function to grow the tree during training\n",
        "    def __grow(self,node,D,level):       \n",
        "        #are we in a leaf node? let's do some check...\n",
        "        depth = (self.max_depth is None) or (self.max_depth >= (level+1))\n",
        "        msamp = (self.min_samples_split <= D.shape[0])\n",
        "        n_cls = np.unique(D[:,-1]).shape[0] != 1\n",
        "        \n",
        "        #not a leaf node\n",
        "        if depth and msamp and n_cls:\n",
        "        \n",
        "            #initialize the function parameters\n",
        "            ip_node = None\n",
        "            feature = None\n",
        "            split   = None\n",
        "            left_D  = None\n",
        "            right_D = None\n",
        "            #iterrate through the possible feature/split combinations\n",
        "            for f in range(D.shape[1]-1):\n",
        "                for s in np.unique(D[:,f]):\n",
        "                    #for the current (f,s) combination, split the dataset\n",
        "                    D_l = D[D[:,f]<=s]\n",
        "                    D_r = D[D[:,f]>s]\n",
        "                    #ensure we have non-empty arrays\n",
        "                    if D_l.size and D_r.size:\n",
        "                        #calculate the impurity\n",
        "                        ip  = (D_l.shape[0]/D.shape[0])*self._impurity(D_l) + (D_r.shape[0]/D.shape[0])*self._impurity(D_r)\n",
        "                        #now update the impurity and choice of (f,s)\n",
        "                        if (ip_node is None) or (ip < ip_node):\n",
        "                            ip_node = ip\n",
        "                            feature = f\n",
        "                            split   = s\n",
        "                            left_D  = D_l\n",
        "                            right_D = D_r\n",
        "            #set the current node's parameters\n",
        "            node.set_params(split,feature)\n",
        "            #declare child nodes\n",
        "            left_node  = Node()\n",
        "            right_node = Node()\n",
        "            node.set_children(left_node,right_node)\n",
        "            #investigate child nodes\n",
        "            self.__grow(node.get_left_node(),left_D,level+1)\n",
        "            self.__grow(node.get_right_node(),right_D,level+1)\n",
        "                        \n",
        "        #is a leaf node\n",
        "        else:\n",
        "            \n",
        "            #set the node value & return\n",
        "            node.leafv = self._leaf_value(D)\n",
        "            return\n",
        "        \n",
        "    #private recursive function to traverse the (trained) tree\n",
        "    def __traverse(self,node,Xrow):\n",
        "        #check if we're in a leaf node?\n",
        "        if node.leafv is None:\n",
        "            #get parameters at the node\n",
        "            (s,f) = node.get_params()\n",
        "            #decide to go left or right?\n",
        "            if (Xrow[f] <= s):\n",
        "                return(self.__traverse(node.get_left_node(),Xrow))\n",
        "            else:\n",
        "                return(self.__traverse(node.get_right_node(),Xrow))\n",
        "        else:\n",
        "            #return the leaf value\n",
        "            return(node.leafv)\n",
        "      \n",
        "    #train the tree model\n",
        "    def train(self,Xin,Yin):\n",
        "        #prepare the input data\n",
        "        D = np.concatenate((Xin,Yin.reshape(-1,1)),axis=1)\n",
        "        #set the root node of the tree\n",
        "        self.tree = Node()\n",
        "        #build the tree\n",
        "        self.__grow(self.tree,D,1)\n",
        "        \n",
        "    #make predictions from the trained tree\n",
        "    def predict(self,Xin):\n",
        "        #iterrate through the rows of Xin\n",
        "        p = []\n",
        "        for r in range(Xin.shape[0]):\n",
        "            p.append(self.__traverse(self.tree,Xin[r,:]))\n",
        "        #return predictions\n",
        "        return(np.array(p).flatten())"
      ],
      "metadata": {
        "id": "7I8myYGWrUJX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree Classifier\n",
        "class DecisionTreeClassifier(DecisionTree):\n",
        "    #initializer\n",
        "    def __init__(self,max_depth=2,min_samples_split=5,loss='gini'):\n",
        "        DecisionTree.__init__(self,max_depth,min_samples_split)\n",
        "        self.loss = loss   \n",
        "    \n",
        "    #private function to define the gini impurity\n",
        "    def __gini(self,D):\n",
        "        #initialize the output\n",
        "        G = 0\n",
        "        #iterrate through the unique classes\n",
        "        for c in np.unique(D[:,-1]):\n",
        "            #compute p for the current c\n",
        "            p = D[D[:,-1]==c].shape[0]/D.shape[0]\n",
        "            #compute term for the current c\n",
        "            G += p*(1-p)\n",
        "        #return gini impurity\n",
        "        return(G)\n",
        "    \n",
        "    #private function to define the shannon entropy\n",
        "    def __entropy(self,D):\n",
        "        #initialize the output\n",
        "        H = 0\n",
        "        #iterrate through the unique classes\n",
        "        for c in np.unique(D[:,-1]):\n",
        "            #compute p for the current c\n",
        "            p = D[D[:,-1]==c].shape[0]/D.shape[0]\n",
        "            #compute term for the current c\n",
        "            H -= p*np.log2(p)\n",
        "        #return entropy\n",
        "        return(H)\n",
        "    \n",
        "    #protected function to define the impurity\n",
        "    def _impurity(self,D):\n",
        "        #use the selected loss function to calculate the node impurity\n",
        "        ip = None\n",
        "        if self.loss == 'gini':\n",
        "            ip = self.__gini(D)\n",
        "        elif self.loss == 'entropy':\n",
        "            ip = self.__entropy(D)\n",
        "        #return results\n",
        "        return(ip)\n",
        "    \n",
        "    #protected function to compute the value at a leaf node\n",
        "    def _leaf_value(self,D):\n",
        "         return(stats.mode(D[:,-1])[0])"
      ],
      "metadata": {
        "id": "qe9QVVLh_Cc-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decision Tree Regressor\n",
        "class DecisionTreeRegressor(DecisionTree):\n",
        "    #initializer\n",
        "    def __init__(self,max_depth=None,min_samples_split=2,loss='mse'):\n",
        "        DecisionTree.__init__(self,max_depth,min_samples_split)\n",
        "        self.loss = loss   \n",
        "    \n",
        "    #private function to define the mean squared error\n",
        "    def __mse(self,D):\n",
        "        #compute the mean target for the node\n",
        "        y_m = np.mean(D[:,-1])\n",
        "        #compute the mean squared error wrt the mean\n",
        "        E = np.sum((D[:,-1] - y_m)**2)/D.shape[0]\n",
        "        #return mse\n",
        "        return(E)\n",
        "    \n",
        "    #private function to define the mean absolute error\n",
        "    def __mae(self,D):\n",
        "        #compute the mean target for the node\n",
        "        y_m = np.mean(D[:,-1])\n",
        "        #compute the mean absolute error wrt the mean\n",
        "        E = np.sum(np.abs(D[:,-1] - y_m))/D.shape[0]\n",
        "        #return mae\n",
        "        return(E)\n",
        "    \n",
        "    #protected function to define the impurity\n",
        "    def _impurity(self,D):\n",
        "        #use the selected loss function to calculate the node impurity\n",
        "        ip = None\n",
        "        if self.loss == 'mse':\n",
        "            ip = self.__mse(D)\n",
        "        elif self.loss == 'mae':\n",
        "            ip = self.__mae(D)\n",
        "        #return results\n",
        "        return(ip)\n",
        "    \n",
        "    #protected function to compute the value at a leaf node\n",
        "    def _leaf_value(self,D):\n",
        "         return(np.mean(D[:,-1]))"
      ],
      "metadata": {
        "id": "gVi2kvUt_E3e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Test and compare**\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html"
      ],
      "metadata": {
        "id": "kblGx7zY1kbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test with classification problem"
      ],
      "metadata": {
        "id": "a7Mf7amOVRwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_wine()\n",
        "X    = data.data\n",
        "y    = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "JF-RdDrrrfAd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = DecisionTreeClassifier(min_samples_split=2, max_depth=5)\n",
        "clf.train(X_train,y_train)\n",
        "preds = clf.predict(X_test)\n",
        "print(\"Predicted classes: \", preds)\n",
        "print(\"True classes\",y_test)\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, preds))"
      ],
      "metadata": {
        "id": "z37knQLqAfIX",
        "outputId": "669f30f8-c75f-4e58-8682-72fcf55a0b46",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted classes:  [0. 0. 2. 0. 1. 0. 1. 2. 1. 2. 1. 0. 0. 1. 0. 1. 1. 1. 0. 1. 0. 0. 1. 2.\n",
            " 2. 2. 1. 1. 1. 0. 0. 1. 2. 0. 0. 0.]\n",
            "True classes [0 0 2 0 1 0 1 2 1 2 0 2 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0]\n",
            "Test accuracy:  0.9166666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Result of Decision Tree classifier from the scikit-learn library"
      ],
      "metadata": {
        "id": "_vB_Cl8Fojii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "clf = DecisionTreeClassifier(min_samples_split=2, max_depth=5)  # Create a new classifier object\n",
        "clf.fit(X_train, y_train)       # Train the classifier on the training data\n",
        "pred = clf.predict(X_test)      # Use the classifier to make predictions on the test data\n",
        "\n",
        "print(\"Predicted classes: \", pred)\n",
        "print(\"True classes\",y_test)\n",
        "print(\"Test accuracy: \", accuracy_score(y_test, pred))"
      ],
      "metadata": {
        "id": "R-N72Yhqo4cf",
        "outputId": "159123e5-b07b-4946-b83d-541d6ba7eb2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted classes:  [0 0 2 0 1 0 1 2 1 2 1 0 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0]\n",
            "True classes [0 0 2 0 1 0 1 2 1 2 0 2 0 1 0 1 1 1 0 1 0 1 1 2 2 2 1 1 1 0 0 1 2 0 0 0]\n",
            "Test accuracy:  0.9444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test with regression problem"
      ],
      "metadata": {
        "id": "X6WU_P1fVh7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = make_regression(n_samples=2000, n_features=5, n_informative=5, n_targets=1, noise=1, random_state=42)\n",
        "## do train/test split ##\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "rMPhMcHAY_MH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## declare the regressor and train the model ##\n",
        "dt = DecisionTreeRegressor(max_depth=5,loss='mae')\n",
        "dt.train(X_train,y_train)\n",
        "## make predictions ##\n",
        "yp = dt.predict(X_test)"
      ],
      "metadata": {
        "id": "RByQb6p3ZAH1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## evaluate model performance ##\n",
        "print(\"rmse: %.2f\" % np.sqrt(mean_squared_error(y_test,yp)))\n",
        "print(\"mae: %.2f\" % mean_absolute_error(y_test,yp))\n",
        "print(\"r2: %.2f\" % r2_score(y_test,yp))"
      ],
      "metadata": {
        "id": "i51VMfhjZH1_",
        "outputId": "c8c91fba-3004-4d81-a435-d432e5fa68fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse: 27.65\n",
            "mae: 22.35\n",
            "r2: 0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Result of Decision Tree regressor from the scikit-learn library"
      ],
      "metadata": {
        "id": "fYtxwAPRprn5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "dt = DecisionTreeRegressor()  # Create a new regressor object\n",
        "dt.fit(X_train, y_train)      # Train the regressor on the training data\n",
        "pred = dt.predict(X_test)     # Use the regressor to make predictions on the test data"
      ],
      "metadata": {
        "id": "-OwaCJe1pvPX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"rmse: %.2f\" % np.sqrt(mean_squared_error(y_test,pred)))\n",
        "print(\"mae: %.2f\" % mean_absolute_error(y_test,pred))\n",
        "print(\"r2: %.2f\" % r2_score(y_test,pred))"
      ],
      "metadata": {
        "id": "C2IG--pxp-Br",
        "outputId": "d79268e3-3cf5-4c8e-83c0-1079108e4257",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rmse: 18.43\n",
            "mae: 14.33\n",
            "r2: 0.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the above results, the prebuild decision tree model from scikit-learn library outforms our self-build decision tree in both classification and regression problem."
      ],
      "metadata": {
        "id": "HNNKk-dvqF_M"
      }
    }
  ]
}